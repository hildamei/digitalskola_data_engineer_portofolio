# -*- coding: utf-8 -*-
"""MinAmountCustomer-Hilda Meiranita Prastika Dewi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVVWwWdiLxtu1W8OTwDHu2NTOD78gQc2
"""

# install java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# install spark (change the version number if needed)
!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz

# unzip the spark file to the current folder
!tar xf spark-3.0.0-bin-hadoop3.2.tgz

# set your spark folder to your system path environment. 
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.0-bin-hadoop3.2"

# install findspark using pip
!pip install -q findspark
!pip install pyspark

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark

from pyspark.sql import SparkSession
from pyspark.sql import functions as func
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType

spark = SparkSession.builder.appName("MinAmount").getOrCreate()

schema = StructType([ \
                     StructField("InvoiceNo", StringType(), True), \
                     StructField("StockCode", StringType(), True), \
                     StructField("Description", StringType(), True), \
                     StructField("Quantity", IntegerType(), True), \
                     StructField("InvoiceData", StringType(), True), \
                     StructField("Amount", FloatType(), True), \
                     StructField("CustomerID", StringType(), True), \
                     StructField("Country", StringType(), True)])

# // Read the file as dataframe
df = spark.read.schema(schema).csv("retail-data-full.csv", sep=';')
df.printSchema()

#Select min amount of each customerID sort by min amount
mincustomer=df.groupBy("CustomerId").min("Amount").orderBy("min(Amount)", ascending=True)
#Collect and print results
results=mincustomer.collect()
for result in results:
  print(result)

spark.stop()